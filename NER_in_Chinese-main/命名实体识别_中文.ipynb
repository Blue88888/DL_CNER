{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e70a58c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lixinrong/miniconda3/envs/CNER/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='/home/lixinrong/alixinrongn/NER_in_Chinese-main/bert', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 3862, 7157, 3683, 6612, 1765, 4157, 1762, 1336, 7305,  680, 7032,\n",
       "         7305,  722, 7313, 4638, 3862, 1818,  511,  102,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0],\n",
       "        [ 101, 6821, 2429,  898, 2255,  988, 3717, 4638, 1300, 4289, 7667, 4507,\n",
       "         1744, 1079,  671, 3837, 4638, 6392, 6369, 2360,  712, 2898, 6392, 6369,\n",
       "         8024, 3146,  702, 2456, 5029, 5408, 5125, 5401, 5445, 2612, 2131,  511,\n",
       "          102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#加载分词器\n",
    "path='/home/lixinrong/alixinrongn/NER_in_Chinese-main/bert'\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "\n",
    "print(tokenizer)\n",
    "\n",
    "#分词测试\n",
    "tokenizer.batch_encode_plus(\n",
    "    [[\n",
    "        '海', '钓', '比', '赛', '地', '点', '在', '厦', '门', '与', '金', '门', '之', '间',\n",
    "        '的', '海', '域', '。'\n",
    "    ],\n",
    "     [\n",
    "         '这', '座', '依', '山', '傍', '水', '的', '博', '物', '馆', '由', '国', '内', '一',\n",
    "         '流', '的', '设', '计', '师', '主', '持', '设', '计', '，', '整', '个', '建', '筑',\n",
    "         '群', '精', '美', '而', '恢', '宏', '。'\n",
    "     ]],\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt',\n",
    "    is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3362a434",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20852,\n",
       " ['海',\n",
       "  '钓',\n",
       "  '比',\n",
       "  '赛',\n",
       "  '地',\n",
       "  '点',\n",
       "  '在',\n",
       "  '厦',\n",
       "  '门',\n",
       "  '与',\n",
       "  '金',\n",
       "  '门',\n",
       "  '之',\n",
       "  '间',\n",
       "  '的',\n",
       "  '海',\n",
       "  '域',\n",
       "  '。'],\n",
       " [0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        #names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "\n",
    "        #在线加载数据集\n",
    "        #dataset = load_dataset(path='peoples_daily_ner', split=split)\n",
    "\n",
    "        #离线加载数据集\n",
    "        dataset = load_from_disk(dataset_path='./data')[split]\n",
    "\n",
    "        #过滤掉太长的句子\n",
    "        def f(data):\n",
    "            return len(data['tokens']) <= 512 - 2\n",
    "\n",
    "        dataset = dataset.filter(f)\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        tokens = self.dataset[i]['tokens']\n",
    "        labels = self.dataset[i]['ner_tags']\n",
    "\n",
    "        return tokens, labels\n",
    "\n",
    "\n",
    "dataset = Dataset('train')\n",
    "\n",
    "tokens, labels = dataset[0]\n",
    "\n",
    "len(dataset), tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e59695a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1303\n",
      "[CLS] 日 本 政 府 制 订 重 建 财 政 的 中 长 期 目 标 和 步 骤 ， 是 以 稳 定 增 长 为 前 提 的 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "tensor([7, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7])\n",
      "input_ids torch.Size([16, 113])\n",
      "token_type_ids torch.Size([16, 113])\n",
      "attention_mask torch.Size([16, 113])\n"
     ]
    }
   ],
   "source": [
    "#数据整理函数\n",
    "def collate_fn(data):\n",
    "    tokens = [i[0] for i in data]\n",
    "    labels = [i[1] for i in data]\n",
    "\n",
    "    inputs = tokenizer.batch_encode_plus(tokens,\n",
    "                                         truncation=True,\n",
    "                                         padding=True,\n",
    "                                         return_tensors='pt',\n",
    "                                         is_split_into_words=True)\n",
    "\n",
    "    lens = inputs['input_ids'].shape[1]\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        labels[i] = [7] + labels[i]\n",
    "        labels[i] += [7] * lens\n",
    "        labels[i] = labels[i][:lens]\n",
    "\n",
    "    return inputs, torch.LongTensor(labels)\n",
    "\n",
    "\n",
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=16,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "#查看数据样例\n",
    "for i, (inputs, labels) in enumerate(loader):\n",
    "    break\n",
    "\n",
    "print(len(loader))\n",
    "print(tokenizer.decode(inputs['input_ids'][0]))\n",
    "print(labels[0])\n",
    "\n",
    "for k, v in inputs.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f90b5b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5974.0416\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 113, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "#加载预训练模型\n",
    "path='/home/lixinrong/alixinrongn/NER_in_Chinese-main/bert'\n",
    "pretrained = AutoModel.from_pretrained(path)\n",
    "\n",
    "#统计参数量\n",
    "print(sum(i.numel() for i in pretrained.parameters()) / 10000)\n",
    "\n",
    "#模型试算\n",
    "#[b, lens] -> [b, lens, 768]\n",
    "pretrained(**inputs).last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3096c294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 113, 8])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义下游模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tuneing = False\n",
    "        self.pretrained = None\n",
    "\n",
    "        self.rnn = torch.nn.GRU(768, 768,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(768, 8)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.tuneing:\n",
    "            out = self.pretrained(**inputs).last_hidden_state\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                out = pretrained(**inputs).last_hidden_state\n",
    "\n",
    "        out, _ = self.rnn(out)\n",
    "\n",
    "        out = self.fc(out).softmax(dim=2)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def fine_tuneing(self, tuneing):\n",
    "        self.tuneing = tuneing\n",
    "        if tuneing:\n",
    "            for i in pretrained.parameters():\n",
    "                i.requires_grad = True\n",
    "\n",
    "            pretrained.train()\n",
    "            self.pretrained = pretrained\n",
    "        else:\n",
    "            for i in pretrained.parameters():\n",
    "                i.requires_grad_(False)\n",
    "\n",
    "            pretrained.eval()\n",
    "            self.pretrained = None\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "model(inputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "942184e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2.0913, -1.7579,  0.2877, -1.2426, -0.4096,  2.2735, -0.9321,  0.0239],\n",
       "         [-0.4818, -0.1800, -0.9919, -0.4544, -1.0347, -1.6106,  0.3101,  0.3549],\n",
       "         [-0.8739, -0.0541,  0.5009,  1.5550,  0.5877, -1.2841, -0.4913,  0.7990],\n",
       "         [ 0.6748,  0.5249, -0.4563,  0.1717,  0.7802, -0.8427, -2.0797, -0.0193],\n",
       "         [-1.0526, -0.6842,  3.4607, -1.2752,  1.9816, -0.9120, -1.4115, -0.3938],\n",
       "         [ 1.2822,  0.9445, -0.6793, -0.0294, -0.3112,  0.2028,  2.2314, -0.5454]]),\n",
       " tensor([1., 1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对计算结果和label变形,并且移除pad\n",
    "def reshape_and_remove_pad(outs, labels, attention_mask):\n",
    "    #变形,便于计算loss\n",
    "    #[b, lens, 8] -> [b*lens, 8]\n",
    "    outs = outs.reshape(-1, 8)\n",
    "    #[b, lens] -> [b*lens]\n",
    "    labels = labels.reshape(-1)\n",
    "\n",
    "    #忽略对pad的计算结果\n",
    "    #[b, lens] -> [b*lens - pad]\n",
    "    select = attention_mask.reshape(-1) == 1\n",
    "    outs = outs[select]\n",
    "    labels = labels[select]\n",
    "\n",
    "    return outs, labels\n",
    "\n",
    "\n",
    "reshape_and_remove_pad(torch.randn(2, 3, 8), torch.ones(2, 3),\n",
    "                       torch.ones(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8dab97e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 16, 2, 16)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取正确数量和总数\n",
    "def get_correct_and_total_count(labels, outs):\n",
    "    #[b*lens, 8] -> [b*lens]\n",
    "    outs = outs.argmax(dim=1)\n",
    "    correct = (outs == labels).sum().item()\n",
    "    total = len(labels)\n",
    "\n",
    "    #计算除了0以外元素的正确率,因为0太多了,包括的话,正确率很容易虚高\n",
    "    select = labels != 0\n",
    "    outs = outs[select]\n",
    "    labels = labels[select]\n",
    "    correct_content = (outs == labels).sum().item()\n",
    "    total_content = len(labels)\n",
    "\n",
    "    return correct, total, correct_content, total_content\n",
    "\n",
    "\n",
    "get_correct_and_total_count(torch.ones(16), torch.randn(16, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bd44a7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354.9704\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "\n",
    "#训练\n",
    "def train(epochs):\n",
    "    lr = 2e-5 if model.tuneing else 5e-4\n",
    "\n",
    "    #训练\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for step, (inputs, labels) in enumerate(loader):\n",
    "            #模型计算\n",
    "            #[b, lens] -> [b, lens, 8]\n",
    "            outs = model(inputs)\n",
    "\n",
    "            #对outs和label变形,并且移除pad\n",
    "            #outs -> [b, lens, 8] -> [c, 8]\n",
    "            #labels -> [b, lens] -> [c]\n",
    "            outs, labels = reshape_and_remove_pad(outs, labels,\n",
    "                                                  inputs['attention_mask'])\n",
    "\n",
    "            #梯度下降\n",
    "            loss = criterion(outs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if step % 50 == 0:\n",
    "                counts = get_correct_and_total_count(labels, outs)\n",
    "\n",
    "                accuracy = counts[0] / counts[1]\n",
    "                accuracy_content = counts[2] / counts[3]\n",
    "\n",
    "                print(epoch, step, loss.item(), accuracy, accuracy_content)\n",
    "\n",
    "        torch.save(model, 'model/命名实体识别_中文.model')\n",
    "\n",
    "\n",
    "model.fine_tuneing(False)\n",
    "print(sum(p.numel() for p in model.parameters()) / 10000)\n",
    "#train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc02d6dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6329.012\n"
     ]
    }
   ],
   "source": [
    "model.fine_tuneing(True)\n",
    "print(sum(p.numel() for p in model.parameters()) / 10000)\n",
    "#train(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "622edfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0.9898153221387823 0.9519641332194705\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "def test():\n",
    "    model_load = torch.load('/home/lixinrong/alixinrongn/NER_in_Chinese-main/model/命名实体识别_中文/命名实体识别_中文.model')\n",
    "    model_load.eval()\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=Dataset('validation'),\n",
    "                                              batch_size=128,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    correct_content = 0\n",
    "    total_content = 0\n",
    "\n",
    "    for step, (inputs, labels) in enumerate(loader_test):\n",
    "        if step == 5:\n",
    "            break\n",
    "        print(step)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            #[b, lens] -> [b, lens, 8] -> [b, lens]\n",
    "            outs = model_load(inputs)\n",
    "\n",
    "        #对outs和label变形,并且移除pad\n",
    "        #outs -> [b, lens, 8] -> [c, 8]\n",
    "        #labels -> [b, lens] -> [c]\n",
    "        outs, labels = reshape_and_remove_pad(outs, labels,\n",
    "                                              inputs['attention_mask'])\n",
    "\n",
    "        counts = get_correct_and_total_count(labels, outs)\n",
    "        correct += counts[0]\n",
    "        total += counts[1]\n",
    "        correct_content += counts[2]\n",
    "        total_content += counts[3]\n",
    "\n",
    "    print(correct / total, correct_content / total_content)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25fe8647",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS][UNK]亲身经历的体会比看电视、读报纸更深，我对中国的认识更深了。[UNK][SEP]\n",
      "[CLS]7·····················中5国6········[SEP]7\n",
      "[CLS]7·····················中5国6········[SEP]7\n",
      "==========================\n",
      "[CLS]首先，要牢固树立多层次的团结观。[SEP]\n",
      "[CLS]7················[SEP]7\n",
      "[CLS]7················[SEP]7\n",
      "==========================\n",
      "[CLS]许多企业不惜重金聘人才、买专利、引技术、改装备，努力做到生产一代、开发一代、研制一代，形成新产品链。[SEP]\n",
      "[CLS]7··················································[SEP]7\n",
      "[CLS]7··················································[SEP]7\n",
      "==========================\n",
      "[CLS]全长1463公里的京沪铁路，是我国客货运输最繁忙的铁路干线，货运和客运密度分别是全国铁路平均水平的5．2倍和3．6倍。[SEP]\n",
      "[CLS]7·········京5沪6铁6路6··············································[SEP]7\n",
      "[CLS]7·········京5沪5················································[SEP]7\n",
      "==========================\n",
      "[CLS]二是认为通电、通水、通气等是房产开发商或房产开发管理部门应事先做好的工作，开发商应承担这笔费用。[SEP]\n",
      "[CLS]7················································[SEP]7\n",
      "[CLS]7················································[SEP]7\n",
      "==========================\n",
      "[CLS][UNK]自己作为一名知识分子，中青年干部，在党的培养下，走上领导岗位，作为一个贫农家庭出身的农村孩子，党交给了自己民政工作这样重要的任务，没有把握好自己，（没有）努力为党为最困难的人工作，而走上了犯罪的道路，自己的教训是沉痛的。[UNK][SEP]\n",
      "[CLS]7················································································································[SEP]7\n",
      "[CLS]7················································································································[SEP]7\n",
      "==========================\n",
      "[CLS]该项目由中国航天保险联合体承保，保险金额为2．35亿美元。[SEP]\n",
      "[CLS]7····中3国4航4天4保4险4联4合4体4················[SEP]7\n",
      "[CLS]7····中3国4航4天4保4险4联4合4体4················[SEP]7\n",
      "==========================\n",
      "[CLS]俄罗斯国家统计委员会主席尤尔科夫及该委员会计算机中心的一些负责人8日被捕。[SEP]\n",
      "[CLS]7俄3罗4斯4国4家4统4计4委4员4会4··尤1尔2科2夫2·····计3算4机4中4心4···········[SEP]7\n",
      "[CLS]7俄3罗4斯4国4家4统4计4委4员4会4··尤1尔2科2夫2·····计3··中4心4···········[SEP]7\n",
      "==========================\n",
      "[CLS]同时，三毛集团自身也快速扩张，企业新创造了3000多个就业岗位，安置了一大批下岗职工。[SEP]\n",
      "[CLS]7···三3毛4集4团4····································[SEP]7\n",
      "[CLS]7···三3毛4集4团4····································[SEP]7\n",
      "==========================\n",
      "[CLS]救助贫困母亲，最易扣人心弦，最能激发爱心。[SEP]\n",
      "[CLS]7·····················[SEP]7\n",
      "[CLS]7·····················[SEP]7\n",
      "==========================\n",
      "[CLS]目前，1·4万多平方米的交通控制指挥中心已投入使用，全支队实现了有线通讯程控化，无线通讯网络化，启用了25个路口的交通信号自动适应控制系统和闯红灯自动拍照系统，开通了34个电视监控点，部分路口安装了具有国际先进水平的vtos视频监控设备，开通了道路交通诱导寻呼台，及时向驾驶员提供交通信息，减少交通堵塞。[SEP]\n",
      "[CLS]7············交3通4控4制4指4挥4中4心4····································································································································[SEP]7\n",
      "[CLS]7························································································································································[SEP]7\n",
      "==========================\n",
      "[CLS]辽大注重外国留学生能力培养辽宁大学自1965年以来，招收外国留学生3000余人。[SEP]\n",
      "[CLS]7辽3大4···········辽3宁4大4学4·······················[SEP]7\n",
      "[CLS]7辽3大4···········辽3宁4大4学4·······················[SEP]7\n",
      "==========================\n",
      "[CLS]法律系6名特困生接受了[UNK]金剑法学发展基金[UNK]提供的每人1000元的助学金。[SEP]\n",
      "[CLS]7法3律4系4·········金3剑4法4学4发4展4基4金4················[SEP]7\n",
      "[CLS]7···········[UNK]3金3剑4法4学4发4展4基4金4················[SEP]7\n",
      "==========================\n",
      "[CLS]黄小目自费办夜校的义举，感动了许多人，也感动了福建省和漳州市教委。[SEP]\n",
      "[CLS]7黄1小2目2····················福5建6省6·漳3州4市4教4委4·[SEP]7\n",
      "[CLS]7黄1小2目2····················福5建6省6·漳3州4市4教4委4·[SEP]7\n",
      "==========================\n",
      "[CLS]温得和克消息：为期三天的南部非洲经济首脑会议17日在纳米比亚首都温得和克开幕。[SEP]\n",
      "[CLS]7温5得6和6克6··········非5洲6··········纳5米6比6亚6··温5得6和6克6···[SEP]7\n",
      "[CLS]7温5得6和6克6··········非5洲6··········纳5米6比6亚6··温5得6和6克6···[SEP]7\n",
      "==========================\n",
      "[CLS]在核试验前后，印度政要一再散布[UNK]中国威胁论[UNK]。[SEP]\n",
      "[CLS]7·······印5度6·······中5国6·····[SEP]7\n",
      "[CLS]7·······印5度6·······中5国6·····[SEP]7\n",
      "==========================\n",
      "[CLS]一旁的李宪立几次想给我们介绍一些情况，都被他这位执著的嫂子制止。[SEP]\n",
      "[CLS]7···李1宪2立2··························[SEP]7\n",
      "[CLS]7···李1宪2立2··························[SEP]7\n",
      "==========================\n",
      "[CLS]又一次找到岗位，又一次领到薪金，宋心莲打心里感到这机会的难得，工作起来更加尽心尽力，很快博得了大家的好评。[SEP]\n",
      "[CLS]7················宋1心2莲2··································[SEP]7\n",
      "[CLS]7················宋1心2莲2··································[SEP]7\n",
      "==========================\n",
      "[CLS]其标准对灯的外形尺寸、启动特性、灯功率、光通量、灯的寿命等都有明确的指标。[SEP]\n",
      "[CLS]7·····································[SEP]7\n",
      "[CLS]7·····································[SEP]7\n",
      "==========================\n",
      "[CLS]新华社北京5月7日电国务院副总理李岚清今天在中南海会见了美国前商务部长芭芭拉·弗兰克林。[SEP]\n",
      "[CLS]7新3华4社4北5京6·····国3务4院4···李1岚2清2···中5南6海6···美3国4前4商4务4部4·芭1芭2拉2·2弗2兰2克2林2·[SEP]7\n",
      "[CLS]7新3华4社4北5京6·····国3务4院4···李1岚2清2···中5南6海6···美3国4·商4务4部4·芭1芭2拉2·2弗2兰2克2林2·[SEP]7\n",
      "==========================\n",
      "[CLS]如果科研单位真正有权力裁减冗员，将部分经费用于为科学家配备助手，基本待遇与公司相当，光是那些博士后的家属就完全可以担当起这一重任。[SEP]\n",
      "[CLS]7·································································[SEP]7\n",
      "[CLS]7·································································[SEP]7\n",
      "==========================\n",
      "[CLS]而要实现这种跨越式发展，必须培养和造就高素质的人才。[SEP]\n",
      "[CLS]7··························[SEP]7\n",
      "[CLS]7··························[SEP]7\n",
      "==========================\n",
      "[CLS]从十九世纪以来西学东渐，于是融会中西成为中国学术界的一个显著的特点。[SEP]\n",
      "[CLS]7················中5···中5国6············[SEP]7\n",
      "[CLS]7················中5···中5国6············[SEP]7\n",
      "==========================\n",
      "[CLS]入针即效，患者顿感疼痛有所减轻。[SEP]\n",
      "[CLS]7················[SEP]7\n",
      "[CLS]7················[SEP]7\n",
      "==========================\n",
      "[CLS]在首都新德里，24日的气温为45．6摄氏度，是过去4年里最热的一天。[SEP]\n",
      "[CLS]7···新5德6里6····························[SEP]7\n",
      "[CLS]7···新5德6里6····························[SEP]7\n",
      "==========================\n",
      "[CLS]诗与书，我自知不济，兄前不敢置喙，所以每当良聚，辄以近喜丹青为遁辞，殊不知兄实真人，素不露相，这次我才得到了亲瞻。[SEP]\n",
      "[CLS]7·························································[SEP]7\n",
      "[CLS]7·························································[SEP]7\n",
      "==========================\n",
      "[CLS]记者近日在北京、上海、河南等地调查了解到，目前国有商业企业经营功能退化严重，特别是改建扩容后的大型商厦，自营比重大幅度降低，纷纷采用招商租赁商厦场地，引厂进店设立专卖等经营方式；自营业务中也多采用代销方式，即由供货厂家垫款供货，商家先售货后结账，货卖不完还可退给厂家。[SEP]\n",
      "[CLS]7·····北5京6·上5海6·河5南6·························································································································[SEP]7\n",
      "[CLS]7·····北5京6·上5海6·河5南6·························································································································[SEP]7\n",
      "==========================\n",
      "[CLS]陈和生说，[UNK]铁硼磁铁直径有1．6米，内径有1．2米，由4000多个小块粘起来，飞出一块将对航天飞机构成灾难性打击，所以美方对其安全性要求特别高。[SEP]\n",
      "[CLS]7陈1和2生2························································美5············[SEP]7\n",
      "[CLS]7陈1和2生2························································美5············[SEP]7\n",
      "==========================\n",
      "[CLS]红十字和红新月国际联合会22日在这里发表的1998年《世界灾难报告》说，自汽车问世一个世纪以来，世界公路交通事故共造成3000万人丧生，每年至少要造成50万人死亡，1000多万人受伤。[SEP]\n",
      "[CLS]7红3十4字4和4红4新4月4国4际4联4合4会4················································································[SEP]7\n",
      "[CLS]7红3十4字4·红3新4月4国4际4联4合4会4················································································[SEP]7\n",
      "==========================\n",
      "[CLS]1965年我当选为西藏自治区人民委员会主席以后，周恩来总理要我在北京安个家，有时住在北京，参与人大常委会的工作，有时去拉萨一个时期，主持自治区人民政府的工作。[SEP]\n",
      "[CLS]7·········西3藏4自4治4区4人4民4委4员4会4·····周1恩2来2·····北5京6········北5京6···人3大4常4委4会4·······拉5萨6·······自5治6区6········[SEP]7\n",
      "[CLS]7·········西3藏4自4治4区4人4民4委4员4会4·····周1恩2来2·····北5京6········北5京6···人3大4常4委4会4·······拉5萨6··················[SEP]7\n",
      "==========================\n",
      "[CLS]同时这位负责人还解释说，电信业有两个特点：服务范围广、技术发展快。[SEP]\n",
      "[CLS]7·································[SEP]7\n",
      "[CLS]7·································[SEP]7\n",
      "==========================\n",
      "[CLS]如今，保定[UNK]四快一无[UNK]交警改革的新机制，让绿色的希望变为金色的现实。[SEP]\n",
      "[CLS]7···保5定6·····························[SEP]7\n",
      "[CLS]7···保5定6·····························[SEP]7\n",
      "==========================\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "def predict():\n",
    "    model_load = torch.load('/home/lixinrong/alixinrongn/NER_in_Chinese-main/model/命名实体识别_中文/命名实体识别_中文.model')\n",
    "    model_load.eval()\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=Dataset('validation'),\n",
    "                                              batch_size=32,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(loader_test):\n",
    "        break\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #[b, lens] -> [b, lens, 8] -> [b, lens]\n",
    "        outs = model_load(inputs).argmax(dim=2)\n",
    "\n",
    "    for i in range(32):\n",
    "        #移除pad\n",
    "        select = inputs['attention_mask'][i] == 1\n",
    "        input_id = inputs['input_ids'][i, select]\n",
    "        out = outs[i, select]\n",
    "        label = labels[i, select]\n",
    "        \n",
    "        #输出原句子\n",
    "        print(tokenizer.decode(input_id).replace(' ', ''))\n",
    "\n",
    "        #输出tag\n",
    "        for tag in [label, out]:\n",
    "            s = ''\n",
    "            for j in range(len(tag)):\n",
    "                if tag[j] == 0:\n",
    "                    s += '·'\n",
    "                    continue\n",
    "                s += tokenizer.decode(input_id[j])\n",
    "                s += str(tag[j].item())\n",
    "\n",
    "            print(s)\n",
    "        print('==========================')\n",
    "\n",
    "\n",
    "predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
